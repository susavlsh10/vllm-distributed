[
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 139.76312099879598,
    "throughput_requests_per_sec": 1.0918993828030936,
    "latency_ms": 915.8353010813395,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.9158353010813395,
    "tokens_generated": 128.0,
    "requests_processed": 1.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 138.7419169215905,
    "throughput_requests_per_sec": 1.0839212259499258,
    "latency_ms": 922.5762685139974,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.9225762685139974,
    "tokens_generated": 128.0,
    "requests_processed": 1.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 140.987822235948,
    "throughput_requests_per_sec": 1.1014673612183437,
    "latency_ms": 907.8798294067383,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.9078798294067383,
    "tokens_generated": 128.0,
    "requests_processed": 1.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 551.3527431516204,
    "throughput_requests_per_sec": 4.3074433058720345,
    "latency_ms": 928.6251068115234,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.9286251068115234,
    "tokens_generated": 512.0,
    "requests_processed": 4.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 531.0265663672042,
    "throughput_requests_per_sec": 4.148645049743783,
    "latency_ms": 964.1702175140381,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.9641702175140381,
    "tokens_generated": 512.0,
    "requests_processed": 4.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 518.2784500230964,
    "throughput_requests_per_sec": 4.049050390805441,
    "latency_ms": 987.8859519958496,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.9878859519958496,
    "tokens_generated": 512.0,
    "requests_processed": 4.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 1038.02720062838,
    "throughput_requests_per_sec": 8.109587504909218,
    "latency_ms": 986.4866733551025,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.9864866733551025,
    "tokens_generated": 1024.0,
    "requests_processed": 8.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 998.8884529631202,
    "throughput_requests_per_sec": 7.803816038774377,
    "latency_ms": 1025.1394907633464,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 1.0251394907633464,
    "tokens_generated": 1024.0,
    "requests_processed": 8.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 948.5902772179568,
    "throughput_requests_per_sec": 7.410861540765287,
    "latency_ms": 1079.4966220855713,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 1.0794966220855713,
    "tokens_generated": 1024.0,
    "requests_processed": 8.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 1928.2188162666585,
    "throughput_requests_per_sec": 15.06420950208327,
    "latency_ms": 1062.1201197306316,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 1.0621201197306316,
    "tokens_generated": 2048.0,
    "requests_processed": 16.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 1802.3734421713134,
    "throughput_requests_per_sec": 14.081042516963386,
    "latency_ms": 1136.2795035044353,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 1.1362795035044353,
    "tokens_generated": 2048.0,
    "requests_processed": 16.0
  },
  {
    "config": {
      "tensor_parallel_size": 1,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 1616.7756828058957,
    "throughput_requests_per_sec": 12.63106002192106,
    "latency_ms": 1266.7187054951985,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 1.2667187054951985,
    "tokens_generated": 2048.0,
    "requests_processed": 16.0
  }
]
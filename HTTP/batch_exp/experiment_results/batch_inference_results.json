[
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 1,
      "seq_len": 512,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 196.94399722964863,
    "throughput_requests_per_sec": 1.53862497835663,
    "latency_ms": 649.9309539794922,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.6499309539794922,
    "tokens_generated": 128,
    "requests_processed": 1
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 1,
      "seq_len": 1024,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 195.21608453398073,
    "throughput_requests_per_sec": 1.5251256604217245,
    "latency_ms": 655.683676401774,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.655683676401774,
    "tokens_generated": 128,
    "requests_processed": 1
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 1,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 195.72622849258457,
    "throughput_requests_per_sec": 1.529111160098317,
    "latency_ms": 653.974692026774,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.653974692026774,
    "tokens_generated": 128,
    "requests_processed": 1
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 4,
      "seq_len": 512,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 758.2774329032648,
    "throughput_requests_per_sec": 5.924042444556756,
    "latency_ms": 675.2146085103353,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.6752146085103353,
    "tokens_generated": 512,
    "requests_processed": 4
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 4,
      "seq_len": 1024,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 745.1400358086321,
    "throughput_requests_per_sec": 5.821406529754938,
    "latency_ms": 687.1191660563151,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.6871191660563151,
    "tokens_generated": 512,
    "requests_processed": 4
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 4,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 722.9641006454489,
    "throughput_requests_per_sec": 5.648157036292569,
    "latency_ms": 708.1956068674723,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.7081956068674723,
    "tokens_generated": 512,
    "requests_processed": 4
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 8,
      "seq_len": 512,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 1465.1722313122598,
    "throughput_requests_per_sec": 11.44665805712703,
    "latency_ms": 698.8939444224039,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.6988939444224039,
    "tokens_generated": 1024,
    "requests_processed": 8
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 8,
      "seq_len": 1024,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 1422.177583816125,
    "throughput_requests_per_sec": 11.110762373563476,
    "latency_ms": 720.0225989023844,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.7200225989023844,
    "tokens_generated": 1024,
    "requests_processed": 8
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 8,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 1337.7691911641566,
    "throughput_requests_per_sec": 10.451321805969974,
    "latency_ms": 765.4534180959066,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.7654534180959066,
    "tokens_generated": 1024,
    "requests_processed": 8
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 512,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 2750.60664584501,
    "throughput_requests_per_sec": 21.48911442066414,
    "latency_ms": 744.5630232493082,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.7445630232493082,
    "tokens_generated": 2048,
    "requests_processed": 16
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 1024,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 2610.1588903022994,
    "throughput_requests_per_sec": 20.391866330486714,
    "latency_ms": 784.6265633900961,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.7846265633900961,
    "tokens_generated": 2048,
    "requests_processed": 16
  },
  {
    "config": {
      "tensor_parallel_size": 2,
      "pipeline_parallel_size": 1,
      "data_parallel_size": 1,
      "batch_size": 16,
      "seq_len": 2048,
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 4096,
      "dtype": "auto",
      "max_num_seqs": 256,
      "max_num_batched_tokens": 8192,
      "gpu_memory_utilization": 0.8,
      "temperature": 0.8,
      "top_p": 0.95,
      "max_tokens": 128,
      "distributed_executor_backend": "ray",
      "node_rank": 0,
      "node_size": 1,
      "master_addr": null,
      "master_port": null,
      "num_runs": 3,
      "warmup_runs": 1,
      "trust_remote_code": false,
      "enforce_eager": false,
      "seed": 42
    },
    "throughput_tokens_per_sec": 2333.3653665095685,
    "throughput_requests_per_sec": 18.229416925856004,
    "latency_ms": 877.7022361755371,
    "memory_usage_gb": 0.0,
    "gpu_utilization": 0.0,
    "run_time_seconds": 0.8777022361755371,
    "tokens_generated": 2048,
    "requests_processed": 16
  }
]
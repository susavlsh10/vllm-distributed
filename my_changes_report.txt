=== Files Changed ===
M	.gitignore
A	TKNP/README.md
A	TKNP/archive/analyze_results.py
A	TKNP/archive/batch_inference.py
A	TKNP/archive/design.md
A	TKNP/archive/run_experiment.py
A	TKNP/batch_exp/README_batch_inference.md
A	TKNP/batch_exp/batch_inference_experiment.py
A	TKNP/batch_exp/experiment_results/batch_inference_results.json
A	TKNP/batch_exp/experiment_results/tp_1_results.json
A	TKNP/batch_exp/run_batch_experiments.sh
A	TKNP/benchmark_inference.py
A	TKNP/design.md
A	TKNP/docker_help.md
A	TKNP/implementation/bench_one_to_many.py
A	TKNP/implementation/disaggregated_plan.md
A	TKNP/implementation/distributed_plan.md
A	TKNP/implementation/implementation_plan.md
A	TKNP/implementation/summary.md
A	TKNP/implementation/token_parallel_integration_summary.md
A	TKNP/model_test.py
A	TKNP/prompt_generator.py
A	TKNP/prototype/run_hamp_datacollection.sh
A	TKNP/prototype/run_hamp_tests.sh
A	TKNP/prototype/run_tp_flashattention.sh
A	TKNP/prototype/test_hamp_attention.py
A	TKNP/prototype/tknp_attention.py
A	TKNP/run_tknp_datacollection.sh
A	TKNP/run_tknp_tests.sh
A	TKNP/sbatch_logs/http_tests_4789074_NODES_2/http_tests_4789074_NODES_2.err
A	TKNP/sbatch_logs/http_tests_4789074_NODES_2/http_tests_4789074_NODES_2.out
A	TKNP/sbatch_logs/http_tests_4789411_NODES_1/http_tests_4789411_NODES_1.err
A	TKNP/sbatch_logs/http_tests_4789411_NODES_1/http_tests_4789411_NODES_1.out
A	TKNP/sbatch_logs/http_tests_4790596_NODES_1/http_tests_4790596_NODES_1.err
A	TKNP/sbatch_logs/http_tests_4790596_NODES_1/http_tests_4790596_NODES_1.out
A	TKNP/sbatch_logs/http_tests_4791071_NODES_2/http_tests_4791071_NODES_2.err
A	TKNP/sbatch_logs/http_tests_4791071_NODES_2/http_tests_4791071_NODES_2.out
A	TKNP/sbatch_logs/http_tests_4791126_NODES_2/http_tests_4791126_NODES_2.err
A	TKNP/sbatch_logs/http_tests_4791126_NODES_2/http_tests_4791126_NODES_2.out
A	TKNP/test_TKNP_classes.py
A	TKNP/test_dist_ranks.py
A	TKNP/test_prefix_caching.py
A	TKNP/test_process_groups_only.py
A	TKNP/test_torchrun.py
A	TKNP/tknp_inference_benchmarks.py
M	benchmarks/benchmark_latency.py
A	eval_vllm.py
M	examples/offline_inference/data_parallel.py
M	examples/offline_inference/torchrun_example.py
A	run_jobs.sh
A	start_vllm_container.sh
A	test_sparse_llama.py
A	test_worker_ranks.py
M	vllm/attention/backends/flash_attn.py
M	vllm/config.py
M	vllm/distributed/parallel_state.py
M	vllm/engine/arg_utils.py
M	vllm/engine/llm_engine.py
M	vllm/entrypoints/llm.py
M	vllm/forward_context.py
A	vllm/model_executor/layers/token_parallel_linear.py
A	vllm/model_executor/models/gpt_oss.py
M	vllm/model_executor/models/llama.py
M	vllm/model_executor/models/llama4.py
M	vllm/model_executor/models/llama_eagle.py
M	vllm/model_executor/models/llama_eagle3.py
A	vllm/model_executor/models/llama_og.py
A	vllm/model_executor/models/llama_sparse.py
A	vllm/model_executor/models/llama_vllm.py
A	vllm/model_executor/models/og_llama.py
M	vllm/transformers_utils/configs/ovis.py
M	vllm/v1/attention/backends/flash_attn.py
M	vllm/v1/core/kv_cache_coordinator.py
M	vllm/v1/core/kv_cache_manager.py
M	vllm/v1/core/sched/output.py
M	vllm/v1/core/sched/scheduler.py
M	vllm/v1/engine/llm_engine.py
M	vllm/v1/executor/abstract.py
M	vllm/v1/executor/ray_distributed_executor.py
M	vllm/v1/worker/block_table.py
M	vllm/v1/worker/gpu_model_runner.py
M	vllm/v1/worker/gpu_worker.py
M	vllm/worker/worker.py

=== Statistics ===
 .gitignore                                         |   11 +
 TKNP/README.md                                     |  179 ++++
 TKNP/archive/analyze_results.py                    |  262 +++++
 TKNP/archive/batch_inference.py                    |   79 ++
 TKNP/archive/design.md                             |   27 +
 TKNP/archive/run_experiment.py                     |   56 ++
 TKNP/batch_exp/README_batch_inference.md           |  358 +++++++
 TKNP/batch_exp/batch_inference_experiment.py       |  531 ++++++++++
 .../batch_inference_results.json                   |  434 ++++++++
 .../batch_exp/experiment_results/tp_1_results.json |  434 ++++++++
 TKNP/batch_exp/run_batch_experiments.sh            |  259 +++++
 TKNP/benchmark_inference.py                        |  327 ++++++
 TKNP/design.md                                     |   87 ++
 TKNP/docker_help.md                                |  105 ++
 TKNP/implementation/bench_one_to_many.py           |  490 +++++++++
 TKNP/implementation/disaggregated_plan.md          |  725 ++++++++++++++
 TKNP/implementation/distributed_plan.md            |  570 +++++++++++
 TKNP/implementation/implementation_plan.md         |  242 +++++
 TKNP/implementation/summary.md                     |  409 ++++++++
 .../token_parallel_integration_summary.md          |  162 +++
 TKNP/model_test.py                                 |  174 ++++
 TKNP/prompt_generator.py                           |  414 ++++++++
 TKNP/prototype/run_hamp_datacollection.sh          |  120 +++
 TKNP/prototype/run_hamp_tests.sh                   |   69 ++
 TKNP/prototype/run_tp_flashattention.sh            |   58 ++
 TKNP/prototype/test_hamp_attention.py              |  346 +++++++
 TKNP/prototype/tknp_attention.py                   | 1040 ++++++++++++++++++++
 TKNP/run_tknp_datacollection.sh                    |  120 +++
 TKNP/run_tknp_tests.sh                             |   67 ++
 .../http_tests_4789074_NODES_2.err                 |  322 ++++++
 .../http_tests_4789074_NODES_2.out                 |  128 +++
 .../http_tests_4789411_NODES_1.err                 |  161 +++
 .../http_tests_4789411_NODES_1.out                 |   64 ++
 .../http_tests_4790596_NODES_1.err                 |   20 +
 .../http_tests_4790596_NODES_1.out                 |  161 +++
 .../http_tests_4791071_NODES_2.err                 |  322 ++++++
 .../http_tests_4791071_NODES_2.out                 |  128 +++
 .../http_tests_4791126_NODES_2.err                 |   56 ++
 .../http_tests_4791126_NODES_2.out                 |  655 ++++++++++++
 TKNP/test_TKNP_classes.py                          |  281 ++++++
 TKNP/test_dist_ranks.py                            |  159 +++
 TKNP/test_prefix_caching.py                        |  168 ++++
 TKNP/test_process_groups_only.py                   |  179 ++++
 TKNP/test_torchrun.py                              |  145 +++
 TKNP/tknp_inference_benchmarks.py                  |  206 ++++
 benchmarks/benchmark_latency.py                    |    2 +
 eval_vllm.py                                       |  161 +++
 examples/offline_inference/data_parallel.py        |    4 +-
 examples/offline_inference/torchrun_example.py     |    4 +-
 run_jobs.sh                                        |   13 +
 start_vllm_container.sh                            |   14 +
 test_sparse_llama.py                               |   27 +
 test_worker_ranks.py                               |   64 ++
 vllm/attention/backends/flash_attn.py              |    2 +
 vllm/config.py                                     |   38 +-
 vllm/distributed/parallel_state.py                 |  147 ++-
 vllm/engine/arg_utils.py                           |   29 +
 vllm/engine/llm_engine.py                          |    2 +-
 vllm/entrypoints/llm.py                            |    4 +-
 vllm/forward_context.py                            |   28 +
 .../model_executor/layers/token_parallel_linear.py |  554 +++++++++++
 vllm/model_executor/models/gpt_oss.py              |  614 ++++++++++++
 vllm/model_executor/models/llama.py                |  117 ++-
 vllm/model_executor/models/llama4.py               |    2 +-
 vllm/model_executor/models/llama_eagle.py          |    2 +-
 vllm/model_executor/models/llama_eagle3.py         |    2 +-
 vllm/model_executor/models/llama_og.py             |  652 ++++++++++++
 vllm/model_executor/models/llama_sparse.py         |  700 +++++++++++++
 vllm/model_executor/models/llama_vllm.py           |  647 ++++++++++++
 vllm/model_executor/models/og_llama.py             |  647 ++++++++++++
 vllm/transformers_utils/configs/ovis.py            |    2 +-
 vllm/v1/attention/backends/flash_attn.py           |    7 +
 vllm/v1/core/kv_cache_coordinator.py               |    9 +-
 vllm/v1/core/kv_cache_manager.py                   |   21 +-
 vllm/v1/core/sched/output.py                       |   12 +
 vllm/v1/core/sched/scheduler.py                    |  364 ++++++-
 vllm/v1/engine/llm_engine.py                       |    1 -
 vllm/v1/executor/abstract.py                       |    5 +-
 vllm/v1/executor/ray_distributed_executor.py       |    1 +
 vllm/v1/worker/block_table.py                      |    6 +
 vllm/v1/worker/gpu_model_runner.py                 |  447 ++++++++-
 vllm/v1/worker/gpu_worker.py                       |   65 +-
 vllm/worker/worker.py                              |   42 +
 83 files changed, 16693 insertions(+), 75 deletions(-)

=== Commits ===
d3d5d82c3 Remove InfiniteBench and vllm-inference-bench directories. These directories are still present in benchmarks-archive branch
ca4f8147a fixed tknp output sync, generation does not get stuck if model produces eos, bug fixed
06da98565 batched inference with token parallel works with chuncked prefill and decode stages
40444736c token parallel works with continuous batching
3740cb1a9 docker help, print throughput in llm.py
b1870441d docker help updated
0aafb0beb docker help updated
165c762ac updated tknp inference benchmark to use llama 8.1 inst by default
d73bb6b55 scheduler v1 completed with token and KV capacity aware scheduling allocation
d83a1da28 configurable batch size and sequence lenght for benchmarking added
7f350a246 cleaned output prints and final commit before merge
150c04565 token parallel kv cache updated: only allocates blocks for assigned requests
13171178d token parallel scheduler added in  scheduler and cleaned scripts
1ce324294 kv cache debug started
2412d300d phase 1 correctness complete
e14a8cd16 Token Parallel Phase 1 Complete, Correct Results
e78da1fc9 moved token parallel scheduling to gpu_model_runner for simplicity
9908c3894 updated TokenParallelCol and Row with alltoallsingle
827875cc2 dummey run and warm up running
b674e8f52 updates
48ddf5c53 readme updated
2828d6325 read me updated for token parallel
5f0890244 decorator +
a395e02ea token parallel decorator added for standard layers
cf81fc351 token parallel classes updated for attention
721b495a2 token parallel class implementation started
90de13fe3 renamed HTTP -> TKNP
14bdbcbac quick start+
8b978e723 quick start
c6e3ca496 added vllm bench scripts with working serve
f233f4d19 vllm inference bench scripts initial code added
13f4db180 benchmark prefill decode scripts added
3099fe52c update git
18c85e35c prefill decode first atempt
5eeb4afe4 Stop tracking results and scrub secrets
2133b46df infinite bench added
91acd60e0 working sparse llama with infinite bench added
df197ac47 fix minor miscalled method (#14327)
b12c4361b Fixes a typo about 'max_decode_seq_len' which causes crashes with cuda graph. (#9285)
aefca509e [Performance] Enable chunked prefill and prefix caching together (#8120)
c903425be [FIX] Fix formatting error in main branch (#1822)
64b5c4258 sparse vllm llama works
899b6c383 vllm sparse llama added
6dc02e096 configurable llama added
76f2f5446 HTTP basic work scripts added
24bc28cb3 sbatch script for inference added
f0b0a6ddd process group for token parallel works right
b8032c1b5 new process group added for token parallel, simple test toke_parallel_linear added for attention
415a662df batch inference experiments added:
15cd200ff new fork created, HTTP built and initialized
